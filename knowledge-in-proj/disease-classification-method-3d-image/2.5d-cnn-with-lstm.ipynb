{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetDecoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, res_channels, out_channels):\n",
    "        super(UNetDecoderBlock, self).__init__()\n",
    "        self.upconv = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(out_channels + res_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, skip):\n",
    "        x = self.upconv(x)\n",
    "        x = torch.cat([x, skip], dim=1)\n",
    "        x = self.conv(x)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FPN(nn.Module):\n",
    "    def __init__(self, input_channels: list, output_channels: list, mask_size = (112, 112)):\n",
    "        super(FPN, self).__init__()\n",
    "        self.mask_size = mask_size\n",
    "        self.convs = nn.ModuleList(\n",
    "            [nn.Sequential(\n",
    "                nn.Conv2d(in_ch, out_ch * 2, kernel_size=3, padding=1), # 384, 64  (batch size = 2*11=22frames)\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.BatchNorm2d(out_ch * 2),  # Changed to BatchNorm2d\n",
    "                nn.Conv2d(out_ch * 2, out_ch, kernel_size=3, padding=1) # 64, 32\n",
    "            ) for in_ch, out_ch in zip(input_channels, output_channels)]\n",
    "        )\n",
    "\n",
    "    def forward(self, xs: list):\n",
    "        # Scale feature maps to the same resolution\n",
    "        hcs = [\n",
    "            F.interpolate(c(x), size=self.mask_size, mode='bilinear', align_corners=False)\n",
    "            for i, (c, x) in enumerate(zip(self.convs, xs))\n",
    "        ]\n",
    "        print(len(hcs))\n",
    "        return torch.cat(hcs, dim=1)\n",
    "        # Concatenate along channel dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _25DCnnRnnSegAux(nn.Module):\n",
    "    '''\n",
    "    2.5 D model : CNN model + RNN with Segmentation head\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 encoder,\n",
    "                 num_classes = 2,\n",
    "                 num_classes_aux = 11,\n",
    "                 n_channels = 3,\n",
    "                 head_3d = \"\",\n",
    "                 n_frames = 1,\n",
    "                 dropout_rate=0,\n",
    "                 segmentation_aux=True):\n",
    "        super().__init__()\n",
    "        #set up\n",
    "        self.encoder = encoder\n",
    "        self.num_features= encoder.num_features\n",
    "        self.num_classes = num_classes\n",
    "        self.num_classes_aux = num_classes_aux\n",
    "        self.n_channels = n_channels\n",
    "        self.head_3d = head_3d\n",
    "        self.n_frames = n_frames\n",
    "        self.dropout = nn.Dropout(p=dropout_rate) if dropout_rate > 0 else nn.Identity()\n",
    "        # layers.\n",
    "        if head_3d == 'lstm':\n",
    "            self.lstm = nn.LSTM(input_size = self.num_features,\n",
    "                                hidden_size = self.num_features // 4,\n",
    "                                batch_first = True,\n",
    "                                bidirectional=True)\n",
    "\n",
    "\n",
    "        self.last_layer = nn.Linear(self.num_features, self.num_classes)\n",
    "\n",
    "        if self.num_classes_aux > 0:\n",
    "            if segmentation_aux:\n",
    "                self.aux_layer = SegmentationHead(self.encoder)\n",
    "            else:\n",
    "                self.aux_layer = nn.Linear(self.num_features, self.num_classes_aux)\n",
    "\n",
    "\n",
    "    def extract_features(self, x):\n",
    "        feature_output = self.encoder(x)\n",
    "        intermediate_feature = None\n",
    "        if self.segmentation_aux and 'coat' in self.encoder.name:\n",
    "            layer0_encoder_feature = self.encoder.stem(x)\n",
    "            layer1_encoder_feature = self.encoder.stages[0](layer0_encoder_feature)\n",
    "            layer2_encoder_feature = self.encoder.stages[1](layer1_encoder_feature)\n",
    "            layer3_encoder_feature = self.encoder.stages[2](layer2_encoder_feature)\n",
    "            layer4_encoder_feature = self.encoder.stages[3](layer3_encoder_feature)\n",
    "\n",
    "            intermediate_features = [\n",
    "                layer4_encoder_feature,\n",
    "                layer3_encoder_feature,\n",
    "                layer2_encoder_feature,\n",
    "                layer1_encoder_feature,\n",
    "                layer0_encoder_feature\n",
    "                ]\n",
    "\n",
    "        fts = self.dropout(fts)\n",
    "        return fts, intermediate_features\n",
    "\n",
    "    def forward_head3d(self, x):\n",
    "        if self.head_3d == 'lstm':\n",
    "            x, _ = self.lstm(x)\n",
    "            mean  = x.mean(1)\n",
    "            max_ = x.amax(1)\n",
    "            x = torch.cat([mean, max_], dim=1)\n",
    "        return x\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.head_3d:\n",
    "            bs, n_frames, c, h , w  = x.size()\n",
    "            x = x.view(bs*n_frames, c, h, w)\n",
    "\n",
    "        fts, intermediate_features = self.extract_features(x)\n",
    "\n",
    "        print(\"fts size: \", fts.shape)\n",
    "\n",
    "        if self.head_3d != \"\":\n",
    "            fts = fts.view(bs, n_frames, -1)\n",
    "            fts = self.forward_head3d(fts)\n",
    "\n",
    "        output = self.last_layer(fts)\n",
    "\n",
    "        if self.num_classes_aux:\n",
    "            segmentation_output = self.segmentation_head(intermediate_features)\n",
    "        else:\n",
    "            segmentation_output = None\n",
    "        if self.num_classes_aux > 0:\n",
    "            aux_output = self.aux_layer(fts)\n",
    "        else:\n",
    "            aux_output = torch.zeros((fts.size(0)))\n",
    "        # processing volumn mask into slice -> input to the model\n",
    "\n",
    "        return output, aux_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomLoss, self).__init__()\n",
    "        self.bce = nn.BCEWithLogitsLoss()\n",
    "        self.dice = smp.losses.DiceLoss(smp.losses.MULTILABEL_MODE, from_logits=True)\n",
    "\n",
    "    def forward(self, outputs, targets, masks_outputs, masks_targets):\n",
    "        loss1 = self.bce(outputs, targets)\n",
    "\n",
    "        masks_outputs = masks_outputs.float()\n",
    "\n",
    "        masks_targets = masks_targets.float().flatten(0, 1)\n",
    "\n",
    "        loss2 = self.dice(masks_outputs, masks_targets) #+ self.dice(masks_outputs2, masks_targets)\n",
    "\n",
    "\n",
    "        loss = loss1 + (loss2 * CFG.segw)\n",
    "\n",
    "        return loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aio-hw2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
